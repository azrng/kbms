import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as s,d as e,o as n}from"./app-OPlDaE8U.js";const l="/kbms/ai/1350373-20250101211657228-597289635.png",t="/kbms/ai/1350373-20250101211645377-136082156.png",r="/kbms/ai/1350373-20250101211713251-1229119748.png",h="/kbms/ai/1350373-20250101211719148-106168135.png",p="/kbms/ai/image-20250209223838276.png",d="/kbms/ai/1350373-20250101211727860-1434857526.png",k="/kbms/ai/1350373-20250101211733926-719309424.png",o="/kbms/ai/1350373-20250101211740901-1045422565.png",c="/kbms/ai/1350373-20250101211747720-1536757240.png",g="/kbms/ai/1350373-20250101211759755-427349766.png",m="/kbms/ai/1350373-20250101211804832-1218856794.png",b="/kbms/ai/1350373-20250101211812682-782853015.png",y="/kbms/ai/1350373-20250101211817787-736010266.png",u="/kbms/ai/1350373-20250101211823747-123863987.png",f="/kbms/ai/1350373-20250101211830196-829366208.png",A="/kbms/ai/1350373-20250101211834390-747781163.png",v={};function F(x,i){return n(),s("div",null,i[0]||(i[0]=[e(`<h2 id="应用" tabindex="-1"><a class="header-anchor" href="#应用"><span>应用</span></a></h2><p>1、ollama部署大模型，提供了cli命令行和api，还可以使用maxkb作为用户界面进行使用。</p><h2 id="ollama" tabindex="-1"><a class="header-anchor" href="#ollama"><span>Ollama</span></a></h2><p>Ollama是一款开源的大模型管理工具，它允许用户在本地便捷地运行多种大型开源模型，包括清华大学的ChatGLM、阿里的千问以及Meta的llama等。目前，Ollama兼容macOS、Linux和Windows三大主流操作系统。</p><p>官网：<a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">https://ollama.com/</a></p><h3 id="命令行" tabindex="-1"><a class="header-anchor" href="#命令行"><span>命令行</span></a></h3><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 查询模型列表</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> list</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 运行模型</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> xxx</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 只拉取不运行</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> pull</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> xxx</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 查看当前运行的模型</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> ps</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 停止某一个模型</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> stop</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> xxx</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 删除模型</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> rm</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> xxx</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="部署" tabindex="-1"><a class="header-anchor" href="#部署"><span>部署</span></a></h3><h4 id="windows部署" tabindex="-1"><a class="header-anchor" href="#windows部署"><span>Windows部署</span></a></h4><p>从Ollama官网下载安装程序，按照安装向导完成安装</p><h4 id="linux部署" tabindex="-1"><a class="header-anchor" href="#linux部署"><span>Linux部署</span></a></h4><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">curl</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> https://ollama.ai/install.sh</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> | </span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="docker部署" tabindex="-1"><a class="header-anchor" href="#docker部署"><span>docker部署</span></a></h4><p>这里我们直接使用docker部署的Ollama，我直接放我的docker-compose文件配置</p><div class="language-yaml line-numbers-mode" data-highlighter="shiki" data-ext="yaml" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">services</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  ollama</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    container_name</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">ollama</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    image</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">registry.cn-hangzhou.aliyuncs.com/zrng/ollama:0.4.6</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # docker.io/ollama/ollama:latest</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    ports</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">      - </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">11434:11434</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # 对外端口</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    restart</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">always</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    environment</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">      - </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">OLLAMA_PROXY_URL=http://host.docker.internal:11434/v1</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # web ui使用的时候地址填写：http://host.docker.internal:11434</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    volumes</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">      - </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">E:\\Data\\ollama:/root/.ollama</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # 挂载数据</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>访问地址为：<a href="http://localhost:11434" target="_blank" rel="noopener noreferrer">http://IP:11434</a></li><li>image地址配置的是阿里云镜像仓库地址，防止拉取失败</li><li>OLLAMA_PROXY_URL：这个是后面填写API 域名的时候要用的</li><li>volumes这个挂载了我的容器数据</li></ul><p>执行docker-compose命令后，在容器启动正常后访问Ollama地址判断启动是否正常，比如我这里直接访问：<a href="http://localhost:11434/" target="_blank" rel="noopener noreferrer">http://localhost:11434/</a></p><figure><img src="`+l+`" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><h3 id="查看模型列表" tabindex="-1"><a class="header-anchor" href="#查看模型列表"><span>查看模型列表</span></a></h3><p>通过docker方式部署，通过命令进入Ollama容器中，查看是否存在默认的模型</p><div class="language-yaml line-numbers-mode" data-highlighter="shiki" data-ext="yaml" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 查询模型列表</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">ollama list</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+t+`" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><h4 id="安装llama模型" tabindex="-1"><a class="header-anchor" href="#安装llama模型"><span>安装llama模型</span></a></h4><p>现在来安装一个开源模型，我找了一个小一点的模型llama3.2进行测试，也可以去模型仓库中寻找合适的模型：<a href="https://ollama.com/library" target="_blank" rel="noopener noreferrer">https://ollama.com/library</a></p><div class="language-json line-numbers-mode" data-highlighter="shiki" data-ext="json" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"># 安装大模型</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">ollama run llama</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3.2</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+r+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>安装完成可以通过命令行查看模型是否安装成功</p><figure><img src="'+h+`" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>现在模型已经安装成功，可以在容器内使用命令行使用模型，也可以使用其他UI服务进行使用</p><h4 id="安装deepseek模型" tabindex="-1"><a class="header-anchor" href="#安装deepseek模型"><span>安装Deepseek模型</span></a></h4><p>模型下载地址：<a href="https://ollama.com/library/deepseek-coder" target="_blank" rel="noopener noreferrer">https://ollama.com/library/deepseek-coder</a></p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 拉取模型</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> pull</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> deepseek-r1:1.5b</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> pull</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> deepseek-coder:6.7b</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> pull</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> deepseek-r1:8b</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 运行模型</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> deepseek-r1:1.5b</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> deepseek-coder:6.7b</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> deepseek-r1:8b</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+p+`" alt="image-20250209223838276" tabindex="0" loading="lazy"><figcaption>image-20250209223838276</figcaption></figure><p>其他自定义配置文件可以看：<a href="Ollama%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2DeepSeek-R1:14b%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97">Ollama本地部署DeepSeek-R1:14b完全指南</a></p><h2 id="lm-studio" tabindex="-1"><a class="header-anchor" href="#lm-studio"><span>LM Studio</span></a></h2><p>专注本地大模型的交互，提供用户界面，支持模型的发现、下载、管理、运行。</p><p>官网：<a href="https://lmstudio.ai/" target="_blank" rel="noopener noreferrer">https://lmstudio.ai/</a></p><h3 id="修改源" tabindex="-1"><a class="header-anchor" href="#修改源"><span>修改源</span></a></h3><p>该软件默认使用的模型源是huggingface，因为一些原因访问不方便，但是可以找到安装路径，比如我的安装路径为 D:\\Program Files\\LmStudio\\LM Studio ，那么就可以去\\resources\\app.webpack\\main 找到一个叫做<code>index.js</code>的文件，使用vscode等工具打开该文件，搜索<code>huggingface.co</code> 批量替换为镜像站 <code>hf-mirror.com</code> (先关闭软件后修改)</p><h2 id="vllm" tabindex="-1"><a class="header-anchor" href="#vllm"><span>vLLM</span></a></h2><p>vLLM 是一个快速且易于使用的库，专为大型语言模型 (LLM) 的推理和部署而设计。</p><p>文档：<a href="https://vllm.hyper.ai/" target="_blank" rel="noopener noreferrer">https://vllm.hyper.ai/</a></p><h2 id="cherry-studio" tabindex="-1"><a class="header-anchor" href="#cherry-studio"><span>Cherry Studio</span></a></h2><p>多模型协助的“全能指挥官”</p><p>官网：<a href="https://cherry-ai.com" target="_blank" rel="noopener noreferrer">https://cherry-ai.com</a></p><h3 id="独特优势" tabindex="-1"><a class="header-anchor" href="#独特优势"><span>独特优势</span></a></h3><ul><li>多模型自由切换：支持OpenAI、DeepSeek、Gemini等主流云端模型，同时集成Ollama本地部署，实现云端与本地模型的灵活调用。</li><li>内置300+预配置助手：覆盖写作、编程、设计等场景，用户可自定义助手角色与功能，还能在同一对话中对比多个模型输出结果。</li><li>多模态文件处理：支持文本、PDF、图像等多种格式，集成WebDAV文件管理、代码高亮与Mermaid图表可视化，满足复杂数据处理需求。</li></ul><h3 id="适用场景" tabindex="-1"><a class="header-anchor" href="#适用场景"><span>适用场景：</span></a></h3><ul><li>开发者需多模型对比调试代码或生成文档；</li><li>创作者需快速切换不同风格文案生成；</li><li>企业需兼顾数据隐私（本地模型）与云端高性能模型混合使用。</li></ul><h3 id="选型建议" tabindex="-1"><a class="header-anchor" href="#选型建议"><span>选型建议</span></a></h3><p>适合技术团队、多任务处理者，或对数据隐私与功能扩展性要求较高的用户。</p><h2 id="anythingllm" tabindex="-1"><a class="header-anchor" href="#anythingllm"><span>AnythingLLM</span></a></h2><p>一体化AI应用程序</p><p>官网地址：<a href="https://anythingllm.com" target="_blank" rel="noopener noreferrer">https://anythingllm.com</a></p><p>文档网站：<a href="https://docs.anythingllm.com/" target="_blank" rel="noopener noreferrer">https://docs.anythingllm.com/</a></p><h3 id="独特优势-1" tabindex="-1"><a class="header-anchor" href="#独特优势-1"><span>独特优势</span></a></h3><ul><li>文档智能问答：支持PDF、Word等格式文件索引，通过向量检索技术精准定位文档片段，结合大模型生成上下文关联的答案。</li><li>本地化部署灵活：可对接Ollama等本地推理引擎，无需依赖云端服务，保障敏感数据安全。</li><li>检索与生成一体化：先检索知识库内容，再调用模型生成答案，确保回答的专业性与准确性。</li></ul><h3 id="适用场景-1" tabindex="-1"><a class="header-anchor" href="#适用场景-1"><span>适用场景</span></a></h3><ul><li>企业内部文档库的自动化问答（如员工手册、技术文档）；</li><li>学术研究中的文献摘要与关键信息提取；</li><li>个人用户管理海量笔记或电子书资源。</li></ul><h3 id="选型建议-1" tabindex="-1"><a class="header-anchor" href="#选型建议-1"><span>选型建议</span></a></h3><p>推荐给依赖文档处理的企业、研究机构，或需要构建私有知识库的团队。</p><h2 id="maxkb" tabindex="-1"><a class="header-anchor" href="#maxkb"><span>MaxKB</span></a></h2><p>MaxKB = Max Knowledge Base，是一款基于大语言模型和 RAG 的开源知识库问答系统，广泛应用于智能客服、企业内部知识库、学术研究与教育等场景。</p><p>官网：<a href="https://maxkb.cn/" target="_blank" rel="noopener noreferrer">https://maxkb.cn/</a></p><h3 id="docker部署-1" tabindex="-1"><a class="header-anchor" href="#docker部署-1"><span>docker部署</span></a></h3><p>这个工具我还通过docker工具来创建，还直接放我的docker-compose文件配置</p><div class="language-yaml line-numbers-mode" data-highlighter="shiki" data-ext="yaml" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">services</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  maxkb</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    container_name</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">maxkb</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # http://localhost:28080  admin/MaxKB@123..</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    image</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">registry.cn-hangzhou.aliyuncs.com/zrng/maxkb:1.8.0</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # 1panel/maxkb</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    ports</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">      - </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">28080:8080</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # 对外端口</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    restart</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">always</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>访问地址为：IP+28080</li><li>image地址配置的是阿里云镜像仓库的地址，防止拉取失败</li></ul><p>现在我访问地址：<a href="http://localhost:28080/ui/login" target="_blank" rel="noopener noreferrer">http://localhost:28080</a></p><figure><img src="`+d+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>默认用户名/密码：admin/MaxKB@123.. 官方文档地址为：<a href="https://maxkb.cn/docs/installation/online_installtion/" target="_blank" rel="noopener noreferrer">https://maxkb.cn/docs/installation/online_installtion/</a> ，登录成功后可以按照弹框提示修改默认的密码，然后去系统管理添加模型</p><h3 id="添加模型" tabindex="-1"><a class="header-anchor" href="#添加模型"><span>添加模型</span></a></h3><figure><img src="'+k+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>这里我们可以看到支持很多的大模型</p><figure><img src="'+o+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>因为我的大模型是Ollama，所以选择私有模型=&gt;Ollama，然后添加模型</p><figure><img src="'+c+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><h3 id="添加应用" tabindex="-1"><a class="header-anchor" href="#添加应用"><span>添加应用</span></a></h3><p>现在可以添加应用了，到应用界面添加新应用</p><figure><img src="'+g+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><figure><img src="'+m+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>下面的内容我使用默认的配置，然后点击右上角的保存并发布，然后点击左侧的概览，可以看到应用信息以及访问地址等</p><figure><img src="'+b+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>直接访问地址：<a href="http://localhost:28080/ui/chat/d0a18a63b48e8b94" target="_blank" rel="noopener noreferrer">http://localhost:28080/ui/chat/d0a18a63b48e8b94</a></p><figure><img src="'+y+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>这里可以就可以输入我们要咨询的内容了，根据电脑配置好坏响应内容的速度也有不同。</p><h3 id="嵌入第三方" tabindex="-1"><a class="header-anchor" href="#嵌入第三方"><span>嵌入第三方</span></a></h3><p>通过简单的配置可以将该应用嵌入到第三方系统中</p><figure><img src="'+u+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><h3 id="知识库" tabindex="-1"><a class="header-anchor" href="#知识库"><span>知识库</span></a></h3><p>在知识库选项卡，可以新建知识库并导入文本或Web站点等，然后将我们需要支持咨询的内容上传并向量化</p><figure><img src="'+f+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>然后在应用界面可以关联新建的知识库，以便返回我们更想要的内容。</p><figure><img src="'+A+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><h2 id="ai-dev-gallery" tabindex="-1"><a class="header-anchor" href="#ai-dev-gallery"><span>AI Dev Gallery</span></a></h2><p>大模型展示与分享平台，可以浏览模型以及相关信息，如模型的功能、性能指标、应用场景等。</p><p>项目地址：<a href="https://github.com/microsoft/ai-dev-gallery" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/ai-dev-gallery </a></p><p>介绍文档：<a href="https://mp.weixin.qq.com/s/ZoKZBzJrj490u8rJlZF9qA" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/ZoKZBzJrj490u8rJlZF9qA</a></p><h2 id="dify" tabindex="-1"><a class="header-anchor" href="#dify"><span>Dify</span></a></h2><p>官方文档：<a href="https://enterprise-docs.dify.ai/zh-cn/introduction" target="_blank" rel="noopener noreferrer">https://enterprise-docs.dify.ai/zh-cn/introduction</a></p><p><a href="https://enterprise-docs.dify.ai/zh-cn/deployment/advanced-configuration/dify-helm-chart" target="_blank" rel="noopener noreferrer">Helm文件配置</a> <a href="https://personel-zhouxinle888-a66353926f9185cff28f2bd374a5c3a9dd89d5206.gitlab.io/dify/dify-20.html" target="_blank" rel="noopener noreferrer">Dify文件上传</a></p>',101)]))}const E=a(v,[["render",F]]),C=JSON.parse('{"path":"/ai/modeTools.html","title":"大模型工具","lang":"zh-CN","frontmatter":{"title":"大模型工具","lang":"zh-CN","date":"2025-01-01T00:00:00.000Z","publish":true,"author":"azrng","isOriginal":true,"category":["soft","ai","llm"],"tag":["大模型","工具"],"description":"应用 1、ollama部署大模型，提供了cli命令行和api，还可以使用maxkb作为用户界面进行使用。 Ollama Ollama是一款开源的大模型管理工具，它允许用户在本地便捷地运行多种大型开源模型，包括清华大学的ChatGLM、阿里的千问以及Meta的llama等。目前，Ollama兼容macOS、Linux和Windows三大主流操作系统。 官...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"大模型工具\\",\\"image\\":[\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211657228-597289635.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211645377-136082156.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211713251-1229119748.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211719148-106168135.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/image-20250209223838276.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211727860-1434857526.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211733926-719309424.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211740901-1045422565.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211747720-1536757240.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211759755-427349766.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211804832-1218856794.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211812682-782853015.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211817787-736010266.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211823747-123863987.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211830196-829366208.png\\",\\"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211834390-747781163.png\\"],\\"datePublished\\":\\"2025-01-01T00:00:00.000Z\\",\\"dateModified\\":\\"2025-04-17T13:36:18.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"azrng\\"}]}"],["meta",{"property":"og:url","content":"https://azrng.gitee.io/kbms/kbms/ai/modeTools.html"}],["meta",{"property":"og:site_name","content":"知识库"}],["meta",{"property":"og:title","content":"大模型工具"}],["meta",{"property":"og:description","content":"应用 1、ollama部署大模型，提供了cli命令行和api，还可以使用maxkb作为用户界面进行使用。 Ollama Ollama是一款开源的大模型管理工具，它允许用户在本地便捷地运行多种大型开源模型，包括清华大学的ChatGLM、阿里的千问以及Meta的llama等。目前，Ollama兼容macOS、Linux和Windows三大主流操作系统。 官..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211657228-597289635.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-04-17T13:36:18.000Z"}],["meta",{"property":"article:author","content":"azrng"}],["meta",{"property":"article:tag","content":"工具"}],["meta",{"property":"article:tag","content":"大模型"}],["meta",{"property":"article:published_time","content":"2025-01-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-04-17T13:36:18.000Z"}]]},"git":{"createdTime":1735744124000,"updatedTime":1744896978000,"contributors":[{"name":"azrng","username":"","email":"itzhangyunpeng@163.com","commits":14},{"name":"zhangyunpeng","username":"","email":"zhang.yunpeng@synyi.com","commits":4}]},"readingTime":{"minutes":6.6,"words":1981},"filePathRelative":"ai/modeTools.md","excerpt":"<h2>应用</h2>\\n<p>1、ollama部署大模型，提供了cli命令行和api，还可以使用maxkb作为用户界面进行使用。</p>\\n<h2>Ollama</h2>\\n<p>Ollama是一款开源的大模型管理工具，它允许用户在本地便捷地运行多种大型开源模型，包括清华大学的ChatGLM、阿里的千问以及Meta的llama等。目前，Ollama兼容macOS、Linux和Windows三大主流操作系统。</p>\\n<p>官网：<a href=\\"https://ollama.com/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://ollama.com/</a></p>","autoDesc":true}');export{E as comp,C as data};
