import{_ as l}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,d as a,o as n}from"./app-Du2i1EF2.js";const s={};function t(r,e){return n(),i("div",null,e[0]||(e[0]=[a(`<h2 id="模型平台" tabindex="-1"><a class="header-anchor" href="#模型平台"><span>模型平台</span></a></h2><h3 id="hugging-face" tabindex="-1"><a class="header-anchor" href="#hugging-face"><span>Hugging Face</span></a></h3><p>是一个综合性AI平台，提供了大量的预训练模型，还支持模型训练、微调、部署等功能。适合开发者或研究人员，需要对模型进行深度定制和优化，或者需要使用丰富的工具和库进行开发。</p><p><a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">https://huggingface.co/</a></p><p>国内镜像站：<a href="https://hf-mirror.com/" target="_blank" rel="noopener noreferrer">https://hf-mirror.com/</a> 、<a href="https://www.modelscope.cn/models" target="_blank" rel="noopener noreferrer">https://www.modelscope.cn/</a></p><p>如果想下载GGUF文件搜索的时候记得带上GGUF，例如deepseek-r1-8b-gguf</p><h3 id="ollama-library" tabindex="-1"><a class="header-anchor" href="#ollama-library"><span>Ollama library</span></a></h3><p>提供了大量的预训练模型，专注于模型的部署和使用，适用于希望快速使用预训练模型进行实际应用的用户，降低了使用门槛。适合普通用户或小型团队，希望快速部署和使用预训练模型，而不需要复杂的配置和开发。</p><p>网站：<a href="https://ollama.com/library" target="_blank" rel="noopener noreferrer">https://ollama.com/library</a></p><h2 id="quen" tabindex="-1"><a class="header-anchor" href="#quen"><span>quen</span></a></h2><p>quen2.5-7b(通义千问)</p><h2 id="llama" tabindex="-1"><a class="header-anchor" href="#llama"><span>llama</span></a></h2><p>llama3.2：由 Meta 开发的开源人工智能模型系列</p><h2 id="deepseek" tabindex="-1"><a class="header-anchor" href="#deepseek"><span>DeepSeek</span></a></h2><ul><li><strong>DeepSeek-R1</strong>：专注于逻辑推理、问题解决和教育工具，教育平台、研究工具</li><li><strong>DeepSeek-Coder</strong>：IDE集成、编程平台，专注于编程辅助，包括代码生成、调试和优化</li></ul><h3 id="模型命名解读" tabindex="-1"><a class="header-anchor" href="#模型命名解读"><span>模型命名解读</span></a></h3><p>以下是对“DeepSeek-R1-Distill-Qwen-1.5B-GGUF”和“DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M”模型名称的详细解释：</p><h5 id="_1-deepseek-r1-distill-qwen-1-5b" tabindex="-1"><a class="header-anchor" href="#_1-deepseek-r1-distill-qwen-1-5b"><span>1. <strong>DeepSeek-R1-Distill-Qwen-1.5B</strong></span></a></h5><ul><li><strong>DeepSeek-R1</strong>：这是 DeepSeek 团队开发的模型系列，R1 表示模型版本。</li><li><strong>Distill</strong>：表示该模型是通过知识蒸馏技术从更大的模型（如 DeepSeek-R1）中蒸馏而来。蒸馏过程使模型体积更小，但保留了大部分性能。</li><li><strong>Qwen-1.5B</strong>：表示该模型基于 Qwen 架构，参数量为 15 亿。这种架构在推理任务中表现出色。</li></ul><h5 id="_2-deepseek-r1-distill-qwen-1-5b-gguf" tabindex="-1"><a class="header-anchor" href="#_2-deepseek-r1-distill-qwen-1-5b-gguf"><span>2. <strong>DeepSeek-R1-Distill-Qwen-1.5B-GGUF</strong></span></a></h5><ul><li><strong>GGUF</strong>：表示模型采用了 GGUF 格式（General GPU Format），这是一种高效的模型存储格式，支持多种量化方式。</li><li><strong>适用场景</strong>：这种格式的模型适合在资源受限的环境中运行，例如低配硬件设备。</li></ul><h5 id="_3-deepseek-r1-distill-qwen-1-5b-q4-k-m" tabindex="-1"><a class="header-anchor" href="#_3-deepseek-r1-distill-qwen-1-5b-q4-k-m"><span>3. <strong>DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M</strong></span></a></h5><ul><li><strong>Q4_K_M</strong>：表示模型采用了 4-bit 量化技术（Q4），并通过 K-Means 聚类优化。这种量化方式显著降低了模型的显存占用，同时保持较高的性能。</li><li><strong>适用场景</strong>：这种量化版本适合在显存有限的设备上运行，例如仅使用核显的设备</li></ul><h2 id="mxbai-embed-large" tabindex="-1"><a class="header-anchor" href="#mxbai-embed-large"><span>mxbai-embed-large</span></a></h2><p>mxbai-embed-large:335m： 嵌入模型</p><ul><li>适用于高精度短文本处理、文本分类、情感分析、问答系统等。</li><li>是 RAG 应用的理想选择，适合结合生成模型构建高效的检索增强系统</li></ul><h2 id="all-minilm" tabindex="-1"><a class="header-anchor" href="#all-minilm"><span>all-MiniLM</span></a></h2><p>文本生成嵌入变量（Text Embedding with Variables）是一种将文本数据转换为数值向量（嵌入向量）的技术，通常用于自然语言处理（NLP）任务。嵌入变量的原理基于将文本中的单词、短语或句子映射到高维空间中的点（向量），使得语义相似的文本在向量空间中距离更近。</p><p><strong>适用任务</strong>：广泛应用于语义搜索、文本聚类、句子相似度计算、信息检索等</p><ul><li>适合需要快速推理和低资源消耗的场景，如实时搜索和推荐系统。</li><li>由于其较小的模型尺寸，适合在边缘设备或移动设备上部署。</li></ul><h3 id="版本说明" tabindex="-1"><a class="header-anchor" href="#版本说明"><span>版本说明</span></a></h3><h4 id="_1-all-minilm-l6-v2" tabindex="-1"><a class="header-anchor" href="#_1-all-minilm-l6-v2"><span>1. <strong>all-MiniLM-L6-v2</strong></span></a></h4><ul><li><strong>特点</strong>：这是一个轻量级模型，具有6层Transformer结构，输出384维的嵌入向量。</li><li><strong>优点</strong>： <ul><li><strong>速度快</strong>：模型较小，适合资源受限的场景。</li><li><strong>推理效率高</strong>：适合快速开发和部署。</li></ul></li><li><strong>适用场景</strong>： <ul><li>需要快速原型开发的项目。</li><li>对计算资源有限制（如在边缘设备或低配置服务器上运行）。</li><li>任务对精度要求不高，但对速度要求高。</li></ul></li></ul><h4 id="_2-all-minilm-l12-v2" tabindex="-1"><a class="header-anchor" href="#_2-all-minilm-l12-v2"><span>2.<strong>all-MiniLM-L12-v2</strong></span></a></h4><ul><li><strong>特点</strong>：具有12层Transformer结构，输出384维的嵌入向量。</li><li><strong>优点</strong>： <ul><li><strong>精度更高</strong>：相比L6版本，L12版本在语义相似性任务上表现更好。</li><li><strong>适合复杂任务</strong>：能够更好地捕捉语义信息。</li></ul></li><li><strong>适用场景</strong>： <ul><li>需要更高精度的语义搜索或文本相似性任务。</li><li>有足够的计算资源来支持稍大的模型。</li></ul></li></ul><h3 id="嵌入接口说明" tabindex="-1"><a class="header-anchor" href="#嵌入接口说明"><span>嵌入接口说明</span></a></h3><h4 id="请求参数" tabindex="-1"><a class="header-anchor" href="#请求参数"><span>请求参数</span></a></h4><p>调用 <code>all-MiniLM</code> 模型的嵌入接口时，通常需要以下参数：</p><ul><li><strong><code>model</code></strong>：指定使用的模型名称，例如 <code>&quot;all-MiniLM-L6-v2&quot;</code> 或 <code>&quot;all-MiniLM-L12-v2&quot;</code>。</li><li><strong><code>input</code></strong>：需要嵌入的文本或文本列表。可以是单个字符串，也可以是字符串数组。</li><li><strong><code>truncate</code></strong>（可选）：是否截断输入文本以适应模型的最大上下文长度。默认值为 <code>true</code>。</li><li><strong><code>options</code></strong>（可选）：其他模型参数，具体参数需参考模型文档。</li><li><strong><code>keep_alive</code></strong>（可选）：控制模型在请求完成后保持加载到内存中的时间，默认为 <code>5m</code>。</li></ul><h4 id="调用示例" tabindex="-1"><a class="header-anchor" href="#调用示例"><span>调用示例</span></a></h4><p>url示例：<code>http://localhost:1234/v1/embeddings</code></p><p>单个文本嵌入</p><div class="language-json line-numbers-mode" data-highlighter="shiki" data-ext="json" data-title="json" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  &quot;model&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;all-MiniLM-L6-v2&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  &quot;input&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;This is a sample sentence for embedding.&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>多个文本嵌入</p><div class="language-json line-numbers-mode" data-highlighter="shiki" data-ext="json" data-title="json" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  &quot;model&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;all-MiniLM-L6-v2&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  &quot;input&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: [</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;This is a sample sentence.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;Another sentence for embedding.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="模型部署" tabindex="-1"><a class="header-anchor" href="#模型部署"><span>模型部署</span></a></h2><h3 id="_1、本地部署" tabindex="-1"><a class="header-anchor" href="#_1、本地部署"><span>1、本地部署</span></a></h3><p>本地部署是指将大语言模型直接部署在本地服务器或设备上，适合对数据隐私和安全性要求较高的场景。</p><h4 id="常见工具" tabindex="-1"><a class="header-anchor" href="#常见工具"><span>常见工具：</span></a></h4><ul><li><strong>Ollama</strong>：简化本地部署和运行过程，支持多种预构建模型，可自定义模型。</li><li><strong>LangChain</strong>：结合检索增强生成（RAG）技术，从文档数据库中检索信息后输入到大语言模型，提升生成内容的准确性和相关性。</li><li>使用 LMDeploy 或 TRT-LLM 部署：-企业级推荐 <ul><li>LMDeploy 提供企业级部署方案，支持离线处理和在线服务。 -推荐</li><li>TRT-LLM 支持 BF16 和 INT4/INT8 权重，优化推理速度。</li></ul></li></ul><h4 id="优势" tabindex="-1"><a class="header-anchor" href="#优势"><span>优势：</span></a></h4><ul><li>数据隐私和安全性高。</li><li>可根据需求进行模型微调和优化</li></ul><h3 id="_2、云平台部署" tabindex="-1"><a class="header-anchor" href="#_2、云平台部署"><span>2、云平台部署</span></a></h3><p>通过云服务提供商部署大语言模型，适合需要高可扩展性和灵活性的场景。</p><h4 id="常见工具-1" tabindex="-1"><a class="header-anchor" href="#常见工具-1"><span>常见工具：</span></a></h4><ul><li><strong>Hugging Face Text Generation Inference</strong>：支持在云环境中高效运行大语言模型，提供监控和优化功能。</li><li><strong>DeepSpeed-MII</strong>：支持多副本负载均衡，适合处理大量用户请求。</li></ul><h4 id="优势-1" tabindex="-1"><a class="header-anchor" href="#优势-1"><span>优势：</span></a></h4><ul><li>可按需扩展资源。</li><li>提供丰富的监控和管理工具</li></ul><h3 id="_3、容器化部署" tabindex="-1"><a class="header-anchor" href="#_3、容器化部署"><span>3、容器化部署</span></a></h3><p>使用 Docker 或 Kubernetes 等容器化技术部署大语言模型，适合需要快速部署和灵活管理的场景。</p><h4 id="常见工具-2" tabindex="-1"><a class="header-anchor" href="#常见工具-2"><span>常见工具：</span></a></h4><ul><li><strong>Docker</strong>：通过 Docker 容器快速部署和运行大语言模型。</li><li><strong>Kubernetes</strong>：支持大规模集群管理和弹性扩展。</li></ul><h4 id="优势-2" tabindex="-1"><a class="header-anchor" href="#优势-2"><span>优势：</span></a></h4><ul><li>部署速度快，环境隔离性强。</li><li>易于扩展和管理</li></ul>`,64)]))}const d=l(s,[["render",t],["__file","aiModel.html.vue"]]),p=JSON.parse('{"path":"/ai/aiModel.html","title":"大语言模型","lang":"zh-CN","frontmatter":{"title":"大语言模型","lang":"zh-CN","date":"2025-02-27T00:00:00.000Z","publish":true,"author":"azrng","isOriginal":true,"category":["ai","llm"],"tag":["大模型"],"description":"模型平台 Hugging Face 是一个综合性AI平台，提供了大量的预训练模型，还支持模型训练、微调、部署等功能。适合开发者或研究人员，需要对模型进行深度定制和优化，或者需要使用丰富的工具和库进行开发。 https://huggingface.co/ 国内镜像站：https://hf-mirror.com/ 、https://www.modelsco...","head":[["meta",{"property":"og:url","content":"https://azrng.gitee.io/kbms/kbms/ai/aiModel.html"}],["meta",{"property":"og:site_name","content":"知识库"}],["meta",{"property":"og:title","content":"大语言模型"}],["meta",{"property":"og:description","content":"模型平台 Hugging Face 是一个综合性AI平台，提供了大量的预训练模型，还支持模型训练、微调、部署等功能。适合开发者或研究人员，需要对模型进行深度定制和优化，或者需要使用丰富的工具和库进行开发。 https://huggingface.co/ 国内镜像站：https://hf-mirror.com/ 、https://www.modelsco..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-27T14:01:53.000Z"}],["meta",{"property":"article:author","content":"azrng"}],["meta",{"property":"article:tag","content":"大模型"}],["meta",{"property":"article:published_time","content":"2025-02-27T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-02-27T14:01:53.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"大语言模型\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-02-27T00:00:00.000Z\\",\\"dateModified\\":\\"2025-02-27T14:01:53.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"azrng\\"}]}"]]},"headers":[{"level":2,"title":"模型平台","slug":"模型平台","link":"#模型平台","children":[{"level":3,"title":"Hugging Face","slug":"hugging-face","link":"#hugging-face","children":[]},{"level":3,"title":"Ollama library","slug":"ollama-library","link":"#ollama-library","children":[]}]},{"level":2,"title":"quen","slug":"quen","link":"#quen","children":[]},{"level":2,"title":"llama","slug":"llama","link":"#llama","children":[]},{"level":2,"title":"DeepSeek","slug":"deepseek","link":"#deepseek","children":[{"level":3,"title":"模型命名解读","slug":"模型命名解读","link":"#模型命名解读","children":[{"level":5,"title":"1. DeepSeek-R1-Distill-Qwen-1.5B","slug":"_1-deepseek-r1-distill-qwen-1-5b","link":"#_1-deepseek-r1-distill-qwen-1-5b","children":[]},{"level":5,"title":"2. DeepSeek-R1-Distill-Qwen-1.5B-GGUF","slug":"_2-deepseek-r1-distill-qwen-1-5b-gguf","link":"#_2-deepseek-r1-distill-qwen-1-5b-gguf","children":[]},{"level":5,"title":"3. DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M","slug":"_3-deepseek-r1-distill-qwen-1-5b-q4-k-m","link":"#_3-deepseek-r1-distill-qwen-1-5b-q4-k-m","children":[]}]}]},{"level":2,"title":"mxbai-embed-large","slug":"mxbai-embed-large","link":"#mxbai-embed-large","children":[]},{"level":2,"title":"all-MiniLM","slug":"all-minilm","link":"#all-minilm","children":[{"level":3,"title":"版本说明","slug":"版本说明","link":"#版本说明","children":[{"level":4,"title":"1. all-MiniLM-L6-v2","slug":"_1-all-minilm-l6-v2","link":"#_1-all-minilm-l6-v2","children":[]},{"level":4,"title":"2.all-MiniLM-L12-v2","slug":"_2-all-minilm-l12-v2","link":"#_2-all-minilm-l12-v2","children":[]}]},{"level":3,"title":"嵌入接口说明","slug":"嵌入接口说明","link":"#嵌入接口说明","children":[{"level":4,"title":"请求参数","slug":"请求参数","link":"#请求参数","children":[]},{"level":4,"title":"调用示例","slug":"调用示例","link":"#调用示例","children":[]}]}]},{"level":2,"title":"模型部署","slug":"模型部署","link":"#模型部署","children":[{"level":3,"title":"1、本地部署","slug":"_1、本地部署","link":"#_1、本地部署","children":[{"level":4,"title":"常见工具：","slug":"常见工具","link":"#常见工具","children":[]},{"level":4,"title":"优势：","slug":"优势","link":"#优势","children":[]}]},{"level":3,"title":"2、云平台部署","slug":"_2、云平台部署","link":"#_2、云平台部署","children":[{"level":4,"title":"常见工具：","slug":"常见工具-1","link":"#常见工具-1","children":[]},{"level":4,"title":"优势：","slug":"优势-1","link":"#优势-1","children":[]}]},{"level":3,"title":"3、容器化部署","slug":"_3、容器化部署","link":"#_3、容器化部署","children":[{"level":4,"title":"常见工具：","slug":"常见工具-2","link":"#常见工具-2","children":[]},{"level":4,"title":"优势：","slug":"优势-2","link":"#优势-2","children":[]}]}]}],"git":{"createdTime":1740644838000,"updatedTime":1740664913000,"contributors":[{"name":"azrng","username":"azrng","email":"itzhangyunpeng@163.com","commits":1},{"name":"zhangyunpeng","username":"zhangyunpeng","email":"zhang.yunpeng@synyi.com","commits":1}]},"readingTime":{"minutes":5.77,"words":1730},"filePathRelative":"ai/aiModel.md","localizedDate":"2025年2月27日","excerpt":"<h2>模型平台</h2>\\n<h3>Hugging Face</h3>\\n<p>是一个综合性AI平台，提供了大量的预训练模型，还支持模型训练、微调、部署等功能。适合开发者或研究人员，需要对模型进行深度定制和优化，或者需要使用丰富的工具和库进行开发。</p>\\n<p><a href=\\"https://huggingface.co/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://huggingface.co/</a></p>\\n<p>国内镜像站：<a href=\\"https://hf-mirror.com/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://hf-mirror.com/</a>  、<a href=\\"https://www.modelscope.cn/models\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://www.modelscope.cn/</a></p>","autoDesc":true}');export{d as comp,p as data};
