import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,d as e,o as n}from"./app-Bg64E3Xi.js";const l={};function t(h,s){return n(),a("div",null,s[0]||(s[0]=[e(`<h2 id="模型平台" tabindex="-1"><a class="header-anchor" href="#模型平台"><span>模型平台</span></a></h2><h3 id="在线模型平台" tabindex="-1"><a class="header-anchor" href="#在线模型平台"><span>在线模型平台</span></a></h3><p><a href="https://github.com/marketplace/models" target="_blank" rel="noopener noreferrer">GitHub Models</a></p><p><a href="https://cloud.siliconflow.cn/models" target="_blank" rel="noopener noreferrer">硅基流动</a></p><h3 id="hugging-face" tabindex="-1"><a class="header-anchor" href="#hugging-face"><span>Hugging Face</span></a></h3><p>是一个综合性AI平台，提供了大量的预训练模型，还支持模型训练、微调、部署等功能。适合开发者或研究人员，需要对模型进行深度定制和优化，或者需要使用丰富的工具和库进行开发。</p><p><a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">https://huggingface.co/</a></p><p>国内镜像站：<a href="https://hf-mirror.com/" target="_blank" rel="noopener noreferrer">https://hf-mirror.com/</a> 、<a href="https://www.modelscope.cn/models" target="_blank" rel="noopener noreferrer">https://www.modelscope.cn/</a></p><p>如果想下载GGUF文件搜索的时候记得带上GGUF，例如deepseek-r1-8b-gguf</p><h3 id="ollama-library" tabindex="-1"><a class="header-anchor" href="#ollama-library"><span>Ollama library</span></a></h3><p>提供了大量的预训练模型，专注于模型的部署和使用，适用于希望快速使用预训练模型进行实际应用的用户，降低了使用门槛。适合普通用户或小型团队，希望快速部署和使用预训练模型，而不需要复杂的配置和开发。</p><p>网站：<a href="https://ollama.com/library" target="_blank" rel="noopener noreferrer">https://ollama.com/library</a></p><h2 id="qwen" tabindex="-1"><a class="header-anchor" href="#qwen"><span>qwen</span></a></h2><p>qwen2.5-7b(通义千问)</p><h2 id="llama" tabindex="-1"><a class="header-anchor" href="#llama"><span>llama</span></a></h2><p>llama3.2：由 Meta 开发的开源人工智能模型系列，向量维度值为3072</p><h2 id="deepseek" tabindex="-1"><a class="header-anchor" href="#deepseek"><span>DeepSeek</span></a></h2><ul><li><strong>DeepSeek-R1</strong>：专注于逻辑推理、问题解决和教育工具，教育平台、研究工具</li><li><strong>DeepSeek-Coder</strong>：IDE集成、编程平台，专注于编程辅助，包括代码生成、调试和优化</li></ul><p>DeepSeek生态全家桶来：<a href="https://github.com/deepseek-ai/awesome-deepseek-integration" target="_blank" rel="noopener noreferrer">https://github.com/deepseek-ai/awesome-deepseek-integration</a></p><h3 id="模型命名解读" tabindex="-1"><a class="header-anchor" href="#模型命名解读"><span>模型命名解读</span></a></h3><p>以下是对“DeepSeek-R1-Distill-Qwen-1.5B-GGUF”和“DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M”模型名称的详细解释：</p><h5 id="_1-deepseek-r1-distill-qwen-1-5b" tabindex="-1"><a class="header-anchor" href="#_1-deepseek-r1-distill-qwen-1-5b"><span>1. <strong>DeepSeek-R1-Distill-Qwen-1.5B</strong></span></a></h5><ul><li><strong>DeepSeek-R1</strong>：这是 DeepSeek 团队开发的模型系列，R1 表示模型版本。</li><li><strong>Distill</strong>：表示该模型是通过知识蒸馏技术从更大的模型（如 DeepSeek-R1）中蒸馏而来。蒸馏过程使模型体积更小，但保留了大部分性能。</li><li><strong>Qwen-1.5B</strong>：表示该模型基于 Qwen 架构，参数量为 15 亿。这种架构在推理任务中表现出色。</li></ul><h5 id="_2-deepseek-r1-distill-qwen-1-5b-gguf" tabindex="-1"><a class="header-anchor" href="#_2-deepseek-r1-distill-qwen-1-5b-gguf"><span>2. <strong>DeepSeek-R1-Distill-Qwen-1.5B-GGUF</strong></span></a></h5><ul><li><strong>GGUF</strong>：表示模型采用了 GGUF 格式（General GPU Format），这是一种高效的模型存储格式，支持多种量化方式。</li><li><strong>适用场景</strong>：这种格式的模型适合在资源受限的环境中运行，例如低配硬件设备。</li></ul><h5 id="_3-deepseek-r1-distill-qwen-1-5b-q4-k-m" tabindex="-1"><a class="header-anchor" href="#_3-deepseek-r1-distill-qwen-1-5b-q4-k-m"><span>3. <strong>DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M</strong></span></a></h5><ul><li><strong>Q4_K_M</strong>：表示模型采用了 4-bit 量化技术（Q4），并通过 K-Means 聚类优化。这种量化方式显著降低了模型的显存占用，同时保持较高的性能。</li><li><strong>适用场景</strong>：这种量化版本适合在显存有限的设备上运行，例如仅使用核显的设备</li></ul><h2 id="嵌入模型" tabindex="-1"><a class="header-anchor" href="#嵌入模型"><span>嵌入模型</span></a></h2><p>如何查找中文嵌入模型：<a href="https://blog.csdn.net/2401_85378759/article/details/144737938" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/2401_85378759/article/details/144737938</a></p><h3 id="bge-m3" tabindex="-1"><a class="header-anchor" href="#bge-m3"><span>bge-m3</span></a></h3><p>向量维度1024</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama pull</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> bge-m3</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="conan-embedding" tabindex="-1"><a class="header-anchor" href="#conan-embedding"><span>Conan-Embedding</span></a></h3><p>Conan-Embedding 是一款在中文语义向量评测榜单 C-MTEB 上达到 SOTA（State-of-the-Art）的模型，超越了阿里、百川、OpenAI 等众多 Embedding 模型。它在抑制模型幻觉、增强新热知识表现、提升封闭领域回答能力等方面表现出色。</p><p>是一种专门为中文设计的嵌入模型，其输出维度为 <strong>1792</strong>，通过多尺度表征学习（MRL）技术提升性能</p><h3 id="text2vec" tabindex="-1"><a class="header-anchor" href="#text2vec"><span>Text2vec</span></a></h3><p>Text2vec 是一个功能强大的文本向量化工具，支持多种文本表征模型，如 Word2Vec、Sentence-BERT、CoSENT 和 BGE 等。</p><h3 id="text-embedding-3-large" tabindex="-1"><a class="header-anchor" href="#text-embedding-3-large"><span>text-embedding-3-large</span></a></h3><p>OpenAI的大型嵌入模型，输出维度为3072</p><h3 id="mxbai-embed-large" tabindex="-1"><a class="header-anchor" href="#mxbai-embed-large"><span>mxbai-embed-large</span></a></h3><p>mxbai-embed-large:335m： 嵌入模型</p><ul><li>适用于高精度短文本处理、文本分类、情感分析、问答系统等。</li><li>是 RAG 应用的理想选择，适合结合生成模型构建高效的检索增强系统</li></ul><h3 id="nomic-embed-text" tabindex="-1"><a class="header-anchor" href="#nomic-embed-text"><span>nomic-embed-text</span></a></h3><p><code>nomic-embed-text</code> 模型的嵌入向量维度是可变的。该模型支持自定义维度，范围从 <strong>64 到 768</strong></p><h4 id="调用示例" tabindex="-1"><a class="header-anchor" href="#调用示例"><span>调用示例</span></a></h4><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>url:http://localhost:11434/api/embed</span></span>
<span class="line"><span></span></span>
<span class="line"><span>{</span></span>
<span class="line"><span>    &quot;model&quot;:&quot;nomic-embed-text&quot;,</span></span>
<span class="line"><span>    &quot;input&quot;: &quot;嵌入的文本&quot; </span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>.net调用示例代码</p><div class="language-csharp line-numbers-mode" data-highlighter="shiki" data-ext="csharp" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">private</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> async</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> Task</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&lt;</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">float</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">[]&gt; </span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">GetEmbeddingAsync</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">string</span><span style="--shiki-light:#383A42;--shiki-dark:#E5C07B;"> text</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    var</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> request</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> new</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">        model</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;nomic-embed-text&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">//&quot;text-embedding-3-large&quot;,</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">        input</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> text</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    };</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    var</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> json</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#E5C07B;"> JsonSerializer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">Serialize</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">request</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">);</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    var</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> content</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> new </span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">StringContent</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">json</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#383A42;--shiki-dark:#E5C07B;">Encoding</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#383A42;--shiki-dark:#E5C07B;">UTF8</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;application/json&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">);</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    var</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> response</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> await </span><span style="--shiki-light:#383A42;--shiki-dark:#E5C07B;">_httpClient</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">PostAsync</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">$&quot;{</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">OllamaUrl</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">}/embed&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">content</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">);</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#E5C07B;">    response</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">EnsureSuccessStatusCode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">();</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    var</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> responseJson</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> await </span><span style="--shiki-light:#383A42;--shiki-dark:#E5C07B;">response</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#383A42;--shiki-dark:#E5C07B;">Content</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ReadAsStringAsync</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">();</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    var</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;"> embeddingResponse</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#E5C07B;"> JsonSerializer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">Deserialize</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&lt;</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">EmbeddingResponse</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt;(</span><span style="--shiki-light:#383A42;--shiki-dark:#E06C75;">responseJson</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">);</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#E5C07B;"> embeddingResponse</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#383A42;--shiki-dark:#E5C07B;">Embedding</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">First</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">();</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">public</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> class</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> EmbeddingResponse</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    [</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">JsonPropertyName</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;model&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)] </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">public</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> string</span><span style="--shiki-light:#4078F2;--shiki-dark:#ABB2BF;"> Model</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> { </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">get</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">; </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">set</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">; }</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    [</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">JsonPropertyName</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;embeddings&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)] </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">public</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> List</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&lt;</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">float</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">[]&gt; </span><span style="--shiki-light:#4078F2;--shiki-dark:#ABB2BF;">Embedding</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> { </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">get</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">; </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">set</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">; }</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="all-minilm" tabindex="-1"><a class="header-anchor" href="#all-minilm"><span>all-MiniLM</span></a></h3><p>文本生成嵌入变量（Text Embedding with Variables）是一种将文本数据转换为数值向量（嵌入向量）的技术，通常用于自然语言处理（NLP）任务。嵌入变量的原理基于将文本中的单词、短语或句子映射到高维空间中的点（向量），使得语义相似的文本在向量空间中距离更近。</p><p><strong>适用任务</strong>：广泛应用于语义搜索、文本聚类、句子相似度计算、信息检索等</p><ul><li>适合需要快速推理和低资源消耗的场景，如实时搜索和推荐系统。</li><li>由于其较小的模型尺寸，适合在边缘设备或移动设备上部署。</li></ul><h4 id="_1-all-minilm-l6-v2" tabindex="-1"><a class="header-anchor" href="#_1-all-minilm-l6-v2"><span>1. <strong>all-MiniLM-L6-v2</strong></span></a></h4><ul><li><strong>特点</strong>：这是一个轻量级模型，具有6层Transformer结构，输出384维的嵌入向量。</li><li><strong>优点</strong>： <ul><li><strong>速度快</strong>：模型较小，适合资源受限的场景。</li><li><strong>推理效率高</strong>：适合快速开发和部署。</li></ul></li><li><strong>适用场景</strong>： <ul><li>需要快速原型开发的项目。</li><li>对计算资源有限制（如在边缘设备或低配置服务器上运行）。</li><li>任务对精度要求不高，但对速度要求高。</li></ul></li></ul><h4 id="_2-all-minilm-l12-v2" tabindex="-1"><a class="header-anchor" href="#_2-all-minilm-l12-v2"><span>2.<strong>all-MiniLM-L12-v2</strong></span></a></h4><ul><li><strong>特点</strong>：具有12层Transformer结构，输出384维的嵌入向量。</li><li><strong>优点</strong>： <ul><li><strong>精度更高</strong>：相比L6版本，L12版本在语义相似性任务上表现更好。</li><li><strong>适合复杂任务</strong>：能够更好地捕捉语义信息。</li></ul></li><li><strong>适用场景</strong>： <ul><li>需要更高精度的语义搜索或文本相似性任务。</li><li>有足够的计算资源来支持稍大的模型。</li></ul></li></ul><h4 id="嵌入接口说明" tabindex="-1"><a class="header-anchor" href="#嵌入接口说明"><span>嵌入接口说明</span></a></h4><h5 id="请求参数" tabindex="-1"><a class="header-anchor" href="#请求参数"><span>请求参数</span></a></h5><p>调用 <code>all-MiniLM</code> 模型的嵌入接口时，通常需要以下参数：</p><ul><li><strong><code>model</code></strong>：指定使用的模型名称，例如 <code>&quot;all-MiniLM-L6-v2&quot;</code> 或 <code>&quot;all-MiniLM-L12-v2&quot;</code>。</li><li><strong><code>input</code></strong>：需要嵌入的文本或文本列表。可以是单个字符串，也可以是字符串数组。</li><li><strong><code>truncate</code></strong>（可选）：是否截断输入文本以适应模型的最大上下文长度。默认值为 <code>true</code>。</li><li><strong><code>options</code></strong>（可选）：其他模型参数，具体参数需参考模型文档。</li><li><strong><code>keep_alive</code></strong>（可选）：控制模型在请求完成后保持加载到内存中的时间，默认为 <code>5m</code>。</li></ul><h4 id="调用示例-1" tabindex="-1"><a class="header-anchor" href="#调用示例-1"><span>调用示例</span></a></h4><p>url示例：<code>http://localhost:1234/v1/embeddings</code></p><h5 id="单个文本嵌入" tabindex="-1"><a class="header-anchor" href="#单个文本嵌入"><span>单个文本嵌入</span></a></h5><div class="language-json line-numbers-mode" data-highlighter="shiki" data-ext="json" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  &quot;model&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;all-MiniLM-L6-v2&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  &quot;input&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;This is a sample sentence for embedding.&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>多个文本嵌入</p><div class="language-json line-numbers-mode" data-highlighter="shiki" data-ext="json" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  &quot;model&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;all-MiniLM-L6-v2&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  &quot;input&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: [</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;This is a sample sentence.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;Another sentence for embedding.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="模型部署" tabindex="-1"><a class="header-anchor" href="#模型部署"><span>模型部署</span></a></h2><h3 id="_1、本地部署" tabindex="-1"><a class="header-anchor" href="#_1、本地部署"><span>1、本地部署</span></a></h3><p>本地部署是指将大语言模型直接部署在本地服务器或设备上，适合对数据隐私和安全性要求较高的场景。</p><h4 id="常见工具" tabindex="-1"><a class="header-anchor" href="#常见工具"><span>常见工具：</span></a></h4><ul><li><strong>Ollama</strong>：简化本地部署和运行过程，支持多种预构建模型，可自定义模型。</li><li><strong>LangChain</strong>：结合检索增强生成（RAG）技术，从文档数据库中检索信息后输入到大语言模型，提升生成内容的准确性和相关性。</li><li>使用 LMDeploy 或 TRT-LLM 部署：-企业级推荐 <ul><li>LMDeploy 提供企业级部署方案，支持离线处理和在线服务。 -推荐</li><li>TRT-LLM 支持 BF16 和 INT4/INT8 权重，优化推理速度。</li></ul></li></ul><h4 id="优势" tabindex="-1"><a class="header-anchor" href="#优势"><span>优势：</span></a></h4><ul><li>数据隐私和安全性高。</li><li>可根据需求进行模型微调和优化</li></ul><h3 id="_2、云平台部署" tabindex="-1"><a class="header-anchor" href="#_2、云平台部署"><span>2、云平台部署</span></a></h3><p>通过云服务提供商部署大语言模型，适合需要高可扩展性和灵活性的场景。</p><h4 id="常见工具-1" tabindex="-1"><a class="header-anchor" href="#常见工具-1"><span>常见工具：</span></a></h4><ul><li><strong>Hugging Face Text Generation Inference</strong>：支持在云环境中高效运行大语言模型，提供监控和优化功能。</li><li><strong>DeepSpeed-MII</strong>：支持多副本负载均衡，适合处理大量用户请求。</li></ul><h4 id="优势-1" tabindex="-1"><a class="header-anchor" href="#优势-1"><span>优势：</span></a></h4><ul><li>可按需扩展资源。</li><li>提供丰富的监控和管理工具</li></ul><h3 id="_3、容器化部署" tabindex="-1"><a class="header-anchor" href="#_3、容器化部署"><span>3、容器化部署</span></a></h3><p>使用 Docker 或 Kubernetes 等容器化技术部署大语言模型，适合需要快速部署和灵活管理的场景。</p><h4 id="常见工具-2" tabindex="-1"><a class="header-anchor" href="#常见工具-2"><span>常见工具：</span></a></h4><ul><li><strong>Docker</strong>：通过 Docker 容器快速部署和运行大语言模型。</li><li><strong>Kubernetes</strong>：支持大规模集群管理和弹性扩展。</li></ul><h4 id="优势-2" tabindex="-1"><a class="header-anchor" href="#优势-2"><span>优势：</span></a></h4><ul><li>部署速度快，环境隔离性强。</li><li>易于扩展和管理</li></ul>`,85)]))}const d=i(l,[["render",t]]),k=JSON.parse('{"path":"/ai/aiModel.html","title":"大语言模型","lang":"zh-CN","frontmatter":{"title":"大语言模型","lang":"zh-CN","date":"2025-02-27T00:00:00.000Z","publish":true,"author":"azrng","isOriginal":true,"category":["ai","llm"],"tag":["大模型"],"description":"模型平台 在线模型平台 GitHub Models 硅基流动 Hugging Face 是一个综合性AI平台，提供了大量的预训练模型，还支持模型训练、微调、部署等功能。适合开发者或研究人员，需要对模型进行深度定制和优化，或者需要使用丰富的工具和库进行开发。 https://huggingface.co/ 国内镜像站：https://hf-mirror....","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"大语言模型\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-02-27T00:00:00.000Z\\",\\"dateModified\\":\\"2025-04-12T09:11:45.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"azrng\\"}]}"],["meta",{"property":"og:url","content":"https://azrng.gitee.io/kbms/kbms/ai/aiModel.html"}],["meta",{"property":"og:site_name","content":"知识库"}],["meta",{"property":"og:title","content":"大语言模型"}],["meta",{"property":"og:description","content":"模型平台 在线模型平台 GitHub Models 硅基流动 Hugging Face 是一个综合性AI平台，提供了大量的预训练模型，还支持模型训练、微调、部署等功能。适合开发者或研究人员，需要对模型进行深度定制和优化，或者需要使用丰富的工具和库进行开发。 https://huggingface.co/ 国内镜像站：https://hf-mirror...."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-04-12T09:11:45.000Z"}],["meta",{"property":"article:author","content":"azrng"}],["meta",{"property":"article:tag","content":"大模型"}],["meta",{"property":"article:published_time","content":"2025-02-27T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-04-12T09:11:45.000Z"}]]},"git":{"createdTime":1740644838000,"updatedTime":1744449105000,"contributors":[{"name":"zhangyunpeng","username":"","email":"zhang.yunpeng@synyi.com","commits":4},{"name":"azrng","username":"","email":"itzhangyunpeng@163.com","commits":6}]},"readingTime":{"minutes":6.97,"words":2090},"filePathRelative":"ai/aiModel.md","excerpt":"<h2>模型平台</h2>\\n<h3>在线模型平台</h3>\\n<p><a href=\\"https://github.com/marketplace/models\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">GitHub Models</a></p>\\n<p><a href=\\"https://cloud.siliconflow.cn/models\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">硅基流动</a></p>\\n<h3>Hugging Face</h3>\\n<p>是一个综合性AI平台，提供了大量的预训练模型，还支持模型训练、微调、部署等功能。适合开发者或研究人员，需要对模型进行深度定制和优化，或者需要使用丰富的工具和库进行开发。</p>","autoDesc":true}');export{d as comp,k as data};
