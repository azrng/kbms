import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as h,d as s,e as n,f as e,a as p,g as a,r,o as k}from"./app-BQsqMNmR.js";const d="/kbms/ai/1350373-20250101211657228-597289635.png",o="/kbms/ai/1350373-20250101211645377-136082156.png",c="/kbms/ai/1350373-20250101211713251-1229119748.png",g="/kbms/ai/1350373-20250101211719148-106168135.png",m="/kbms/ai/1350373-20250101211727860-1434857526.png",y="/kbms/ai/1350373-20250101211733926-719309424.png",b="/kbms/ai/1350373-20250101211740901-1045422565.png",u="/kbms/ai/1350373-20250101211747720-1536757240.png",A="/kbms/ai/1350373-20250101211759755-427349766.png",v="/kbms/ai/1350373-20250101211804832-1218856794.png",f="/kbms/ai/1350373-20250101211812682-782853015.png",F="/kbms/ai/1350373-20250101211817787-736010266.png",x="/kbms/ai/1350373-20250101211823747-123863987.png",B="/kbms/ai/1350373-20250101211830196-829366208.png",E="/kbms/ai/1350373-20250101211834390-747781163.png",C={};function _(O,i){const l=r("font");return k(),h("div",null,[i[3]||(i[3]=s(`<h2 id="ollama" tabindex="-1"><a class="header-anchor" href="#ollama"><span>Ollama</span></a></h2><p>Ollama是一款开源的大模型管理工具，它允许用户在本地便捷地运行多种大型开源模型，包括清华大学的ChatGLM、阿里的千问以及Meta的llama等。目前，Ollama兼容macOS、Linux和Windows三大主流操作系统。</p><p>官网：<a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">https://ollama.com/</a></p><h3 id="命令行" tabindex="-1"><a class="header-anchor" href="#命令行"><span>命令行</span></a></h3><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" data-title="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 查询模型列表</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> list</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 运行模型</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> xxx</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 只拉取不运行</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> pull</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> xxx</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 查看当前运行的模型</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> ps</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 停止某一个模型</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> stop</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> xxx</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 删除模型</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">ollama</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> rm</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> xxx</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="部署" tabindex="-1"><a class="header-anchor" href="#部署"><span>部署</span></a></h3><p>这里我们直接使用docker部署的Ollama，我直接放我的docker-compose文件配置</p><div class="language-yaml line-numbers-mode" data-highlighter="shiki" data-ext="yaml" data-title="yaml" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">services</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  ollama</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    container_name</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">ollama</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    image</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">registry.cn-hangzhou.aliyuncs.com/zrng/ollama:0.4.6</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # docker.io/ollama/ollama:latest</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    ports</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">      - </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">11434:11434</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # 对外端口</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    restart</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">always</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    environment</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">      - </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">OLLAMA_PROXY_URL=http://host.docker.internal:11434/v1</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # web ui使用的时候地址填写：http://host.docker.internal:11434</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    volumes</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">      - </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">E:\\Data\\ollama:/root/.ollama</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # 挂载数据</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>访问地址为：<a href="http://localhost:11434" target="_blank" rel="noopener noreferrer">http://IP:11434</a></li><li>image地址配置的是阿里云镜像仓库地址，防止拉取失败</li><li>OLLAMA_PROXY_URL：这个是后面填写API 域名的时候要用的</li><li>volumes这个挂载了我的容器数据</li></ul>`,9)),n(l,{style:{color:"rgb(31, 35, 41)"}},{default:e(()=>i[0]||(i[0]=[a("执行docker-compose命令后，在容器启动正常后访问Ollama地址判断启动是否正常，比如我这里直接访问：")])),_:1}),i[4]||(i[4]=s('<a href="http://localhost:11434/" target="_blank" rel="noopener noreferrer">http://localhost:11434/</a><figure><img src="'+d+`" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>通过命令进入Ollama容器中，查看是否存在默认的模型</p><div class="language-yaml line-numbers-mode" data-highlighter="shiki" data-ext="yaml" data-title="yaml" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 查询模型列表</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">ollama list</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+o+`" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>现在来安装一个开源模型，我找了一个小一点的模型llama3.2进行测试，也可以去模型仓库中寻找合适的模型：<a href="https://ollama.com/library" target="_blank" rel="noopener noreferrer">https://ollama.com/library</a></p><div class="language-json line-numbers-mode" data-highlighter="shiki" data-ext="json" data-title="json" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"># 安装大模型</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">ollama run llama</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3.2</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+c+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>安装完成可以通过命令行查看模型是否安装成功</p><figure><img src="'+g+`" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>现在模型已经安装成功，可以在容器内使用命令行使用模型，也可以使用其他UI服务进行使用</p><h2 id="maxkb" tabindex="-1"><a class="header-anchor" href="#maxkb"><span>MaxKB</span></a></h2><p>MaxKB = Max Knowledge Base，是一款基于大语言模型和 RAG 的开源知识库问答系统，广泛应用于智能客服、企业内部知识库、学术研究与教育等场景。</p><p>官网：<a href="https://maxkb.cn/" target="_blank" rel="noopener noreferrer">https://maxkb.cn/</a></p><h3 id="部署-1" tabindex="-1"><a class="header-anchor" href="#部署-1"><span>部署</span></a></h3><p>这个工具我还通过docker工具来创建，还直接放我的docker-compose文件配置</p><div class="language-yaml line-numbers-mode" data-highlighter="shiki" data-ext="yaml" data-title="yaml" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">services</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  maxkb</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    container_name</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">maxkb</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # http://localhost:28080  admin/MaxKB@123..</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    image</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">registry.cn-hangzhou.aliyuncs.com/zrng/maxkb:1.8.0</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # 1panel/maxkb</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    ports</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">      - </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">28080:8080</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> # 对外端口</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    restart</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">always</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>访问地址为：IP+28080</li><li>image地址配置的是阿里云镜像仓库的地址，防止拉取失败</li></ul><p>现在我访问地址：<a href="http://localhost:28080/ui/login" target="_blank" rel="noopener noreferrer">http://localhost:28080</a></p><figure><img src="`+m+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>默认用户名/密码：admin/MaxKB@123.. 官方文档地址为：<a href="https://maxkb.cn/docs/installation/online_installtion/" target="_blank" rel="noopener noreferrer">https://maxkb.cn/docs/installation/online_installtion/</a> ，登录成功后可以按照弹框提示修改默认的密码，然后去系统管理添加模型</p><h3 id="添加模型" tabindex="-1"><a class="header-anchor" href="#添加模型"><span>添加模型</span></a></h3><figure><img src="'+y+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>这里我们可以看到支持很多的大模型</p><figure><img src="'+b+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure>',25)),p("p",null,[i[2]||(i[2]=a("因为我的模型是")),n(l,{style:{color:"rgb(31, 35, 41)"}},{default:e(()=>i[1]||(i[1]=[a("Ollama，可以选择该私有模型，然后添加模型")])),_:1})]),i[5]||(i[5]=s('<figure><img src="'+u+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><h3 id="添加应用" tabindex="-1"><a class="header-anchor" href="#添加应用"><span>添加应用</span></a></h3><p>现在可以添加应用了，到应用界面添加新应用</p><figure><img src="'+A+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><figure><img src="'+v+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>下面的内容我使用默认的配置，然后点击右上角的保存并发布，然后点击左侧的概览，可以看到应用信息以及访问地址等</p><figure><img src="'+f+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>直接访问地址：<a href="http://localhost:28080/ui/chat/d0a18a63b48e8b94" target="_blank" rel="noopener noreferrer">http://localhost:28080/ui/chat/d0a18a63b48e8b94</a></p><figure><img src="'+F+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>这里可以就可以输入我们要咨询的内容了，根据电脑配置好坏响应内容的速度也有不同。</p><h3 id="嵌入第三方" tabindex="-1"><a class="header-anchor" href="#嵌入第三方"><span>嵌入第三方</span></a></h3><p>通过简单的配置可以将该应用嵌入到第三方系统中</p><figure><img src="'+x+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><h3 id="知识库" tabindex="-1"><a class="header-anchor" href="#知识库"><span>知识库</span></a></h3><p>在知识库选项卡，可以新建知识库并导入文本或Web站点等，然后将我们需要支持咨询的内容上传并向量化</p><figure><img src="'+B+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p>然后在应用界面可以关联新建的知识库，以便返回我们更想要的内容。</p><figure><img src="'+E+'" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure>',18))])}const L=t(C,[["render",_],["__file","localMode.html.vue"]]),w=JSON.parse('{"path":"/ai/localMode.html","title":"本地大模型初体验","lang":"zh-CN","frontmatter":{"title":"本地大模型初体验","lang":"zh-CN","date":"2025-01-01T00:00:00.000Z","publish":true,"author":"azrng","isOriginal":true,"category":["soft"],"tag":["大模型"],"article":false,"description":"Ollama Ollama是一款开源的大模型管理工具，它允许用户在本地便捷地运行多种大型开源模型，包括清华大学的ChatGLM、阿里的千问以及Meta的llama等。目前，Ollama兼容macOS、Linux和Windows三大主流操作系统。 官网：https://ollama.com/ 命令行 部署 这里我们直接使用docker部署的Ollama，...","head":[["meta",{"property":"og:url","content":"https://azrng.gitee.io/kbms/kbms/ai/localMode.html"}],["meta",{"property":"og:site_name","content":"知识库"}],["meta",{"property":"og:title","content":"本地大模型初体验"}],["meta",{"property":"og:description","content":"Ollama Ollama是一款开源的大模型管理工具，它允许用户在本地便捷地运行多种大型开源模型，包括清华大学的ChatGLM、阿里的千问以及Meta的llama等。目前，Ollama兼容macOS、Linux和Windows三大主流操作系统。 官网：https://ollama.com/ 命令行 部署 这里我们直接使用docker部署的Ollama，..."}],["meta",{"property":"og:type","content":"website"}],["meta",{"property":"og:image","content":"https://azrng.gitee.io/kbms/kbms/ai/1350373-20250101211657228-597289635.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-01-04T14:20:36.000Z"}],["meta",{"property":"article:author","content":"azrng"}],["meta",{"property":"article:tag","content":"大模型"}],["meta",{"property":"article:published_time","content":"2025-01-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-01-04T14:20:36.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"WebPage\\",\\"name\\":\\"本地大模型初体验\\",\\"description\\":\\"Ollama Ollama是一款开源的大模型管理工具，它允许用户在本地便捷地运行多种大型开源模型，包括清华大学的ChatGLM、阿里的千问以及Meta的llama等。目前，Ollama兼容macOS、Linux和Windows三大主流操作系统。 官网：https://ollama.com/ 命令行 部署 这里我们直接使用docker部署的Ollama，...\\"}"]]},"headers":[{"level":2,"title":"Ollama","slug":"ollama","link":"#ollama","children":[{"level":3,"title":"命令行","slug":"命令行","link":"#命令行","children":[]},{"level":3,"title":"部署","slug":"部署","link":"#部署","children":[]}]},{"level":2,"title":"MaxKB","slug":"maxkb","link":"#maxkb","children":[{"level":3,"title":"部署","slug":"部署-1","link":"#部署-1","children":[]},{"level":3,"title":"添加模型","slug":"添加模型","link":"#添加模型","children":[]},{"level":3,"title":"添加应用","slug":"添加应用","link":"#添加应用","children":[]},{"level":3,"title":"嵌入第三方","slug":"嵌入第三方","link":"#嵌入第三方","children":[]},{"level":3,"title":"知识库","slug":"知识库","link":"#知识库","children":[]}]}],"git":{"createdTime":1735744124000,"updatedTime":1736000436000,"contributors":[{"name":"azrng","username":"azrng","email":"itzhangyunpeng@163.com","commits":5},{"name":"zhangyunpeng","username":"zhangyunpeng","email":"zhang.yunpeng@synyi.com","commits":1}]},"readingTime":{"minutes":3.46,"words":1038},"filePathRelative":"ai/localMode.md","localizedDate":"2025年1月1日","excerpt":"<h2>Ollama</h2>\\n<p>Ollama是一款开源的大模型管理工具，它允许用户在本地便捷地运行多种大型开源模型，包括清华大学的ChatGLM、阿里的千问以及Meta的llama等。目前，Ollama兼容macOS、Linux和Windows三大主流操作系统。</p>\\n<p>官网：<a href=\\"https://ollama.com/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://ollama.com/</a></p>\\n<h3>命令行</h3>\\n<div class=\\"language-shell line-numbers-mode\\" data-highlighter=\\"shiki\\" data-ext=\\"shell\\" data-title=\\"shell\\" style=\\"--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34\\"><pre class=\\"shiki shiki-themes one-light one-dark-pro vp-code\\"><code><span class=\\"line\\"><span style=\\"--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic\\"># 查询模型列表</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#4078F2;--shiki-dark:#61AFEF\\">ollama</span><span style=\\"--shiki-light:#50A14F;--shiki-dark:#98C379\\"> list</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic\\"># 运行模型</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#4078F2;--shiki-dark:#61AFEF\\">ollama</span><span style=\\"--shiki-light:#50A14F;--shiki-dark:#98C379\\"> run</span><span style=\\"--shiki-light:#50A14F;--shiki-dark:#98C379\\"> xxx</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic\\"># 只拉取不运行</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#4078F2;--shiki-dark:#61AFEF\\">ollama</span><span style=\\"--shiki-light:#50A14F;--shiki-dark:#98C379\\"> pull</span><span style=\\"--shiki-light:#50A14F;--shiki-dark:#98C379\\"> xxx</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic\\"># 查看当前运行的模型</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#4078F2;--shiki-dark:#61AFEF\\">ollama</span><span style=\\"--shiki-light:#50A14F;--shiki-dark:#98C379\\"> ps</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic\\"># 停止某一个模型</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#4078F2;--shiki-dark:#61AFEF\\">ollama</span><span style=\\"--shiki-light:#50A14F;--shiki-dark:#98C379\\"> stop</span><span style=\\"--shiki-light:#50A14F;--shiki-dark:#98C379\\"> xxx</span></span>\\n<span class=\\"line\\"></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic\\"># 删除模型</span></span>\\n<span class=\\"line\\"><span style=\\"--shiki-light:#4078F2;--shiki-dark:#61AFEF\\">ollama</span><span style=\\"--shiki-light:#50A14F;--shiki-dark:#98C379\\"> rm</span><span style=\\"--shiki-light:#50A14F;--shiki-dark:#98C379\\"> xxx</span></span></code></pre>\\n<div class=\\"line-numbers\\" aria-hidden=\\"true\\" style=\\"counter-reset:line-number 0\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>","autoDesc":true}');export{L as comp,w as data};
